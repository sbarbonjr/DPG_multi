from xgboost import XGBClassifier
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, AdaBoostRegressor, RandomForestRegressor
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.datasets import (
    load_iris,
    load_digits,
    load_wine,
    load_breast_cancer,
    load_diabetes,
)

from sklearn.metrics import mean_squared_error
from sklearn.base import is_classifier, is_regressor
from dpg.core import digraph_to_nx, get_dpg, get_dpg_node_metrics, get_dpg_metrics
from dpg.visualizer import plot_dpg

import numpy as np

def select_dataset(name):
    """
    Selects a standard sklearn dataset based on the provided name.

    Args:
    name: The name of the dataset to load.

    Returns:
    The selected dataset.
    """
    datasets = {
        "iris": load_iris(),
        "diabetes": load_diabetes(),
        "digits": load_digits(),
        "wine": load_wine(),
        "cancer": load_breast_cancer(),
        "cancer": load_breast_cancer(),
    }

    return datasets.get(name.lower(), None)


def test_base_multi(model, datasets, n_learners, perc_var, decimal_threshold, file_name=None, plot=False, save_plot_dir="examples/", attribute=None, communities=False, class_flag=False):
    # Load dataset
    dt = select_dataset(datasets)
    
    # Split dataset into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        dt.data, dt.target, test_size=0.3, random_state=42
    )
    
    #from sklear
    #ensemble_classifier = ExtraTreesClassifier(n_estimators=n_learners, random_state=42)
    #ensemble_classifier = RandomForestClassifier(n_estimators=n_learners, random_state=42)
    #ensemble_classifier = AdaBoostClassifier(n_estimators=n_learners, random_state=42)
    #ensemble_classifier = GradientBoostingClassifier(n_estimators=n_learners, random_state=42)
    #ensemble_classifier = BaggingClassifier(n_estimators=n_learners, random_state=42)
    
    # Train XGBClassifier
    ensemble_classifier = model
    ensemble_classifier.fit(X_train, y_train)

    #xgb.plot_tree(ensemble_classifier, num_trees=2)
    #plt.show()

    # Make predictions
    y_pred = ensemble_classifier.predict(X_test)
    
    # Evaluate the model
    if is_classifier(ensemble):
        accuracy = accuracy_score(y_test, y_pred)
        confusion = confusion_matrix(y_test, y_pred)
        classification_rep = classification_report(y_test, y_pred)

        # Print or save the evaluation results
        if file_name is not None:
            with open(file_name, "w") as f:
                f.write(f'Accuracy: {accuracy:.2f}\n')
                f.write('\nConfusion Matrix:\n')
                for i in confusion:
                    f.write(f'{str(i)}\n')
                f.write('\nClassification Report:')
                f.write(classification_rep)
        else:
            print(f'Accuracy: {accuracy:.2f}')
            print('Confusion Matrix:')
            print(confusion)
            print('Classification Report:')
            print(classification_rep)
    else:
        mse = mean_squared_error(y_test, y_pred)
        print(f"Mean Squared Error: {mse:.2f}")
                

    # Extract DPG
    dot = get_dpg(X_train, dt.feature_names, ensemble_classifier, perc_var, decimal_threshold, num_classes=len(np.unique(y_train)))

    # Convert Graphviz Digraph to NetworkX DiGraph
    dpg_model, nodes_list = digraph_to_nx(dot)

    if len(nodes_list) < 2:
        print("Warning: Less than two nodes resulted.")
        return
    
    # Get metrics from the DPG
    df_dpg = get_dpg_metrics(dpg_model, nodes_list)
    df = get_dpg_node_metrics(dpg_model, nodes_list)
    
    # Plot the DPG if requested
    if plot:
        plot_name = (
            datasets
            + "_"
            + ensemble.__class__.__name__
            + "_"
            + "_bl"
            + str(n_learners)
            + "_perc"
            + str(perc_var)
            + "_dec"
            + str(decimal_threshold)
        )

        plot_dpg(
            plot_name,
            dot,
            df,
            df_dpg,
            save_dir=save_plot_dir,
            attribute=attribute,
            communities=communities,
            class_flag=class_flag
        )
    
    return df, df_dpg


n_learners = 3
list_of_ensembles = [#ExtraTreesClassifier(n_estimators=n_learners, random_state=42),
                     #RandomForestClassifier(n_estimators=n_learners, random_state=42),
                     #AdaBoostClassifier(n_estimators=n_learners, random_state=42),
                     #BaggingClassifier(n_estimators=n_learners, random_state=42),
                     
                     #GradientBoostingClassifier(n_estimators=n_learners, random_state=42), #### ERROR                     
                     #AdaBoostRegressor(n_estimators=n_learners, random_state=42),
                     #RandomForestRegressor(n_estimators=n_learners, random_state=42)

                     XGBClassifier(n_estimators=n_learners, random_state=42)
                         ]

for ensemble in list_of_ensembles:
    class_name = ensemble.__class__.__name__
    print(f"The class name is: {class_name}")

    df, df_dpg_metrics = test_base_multi(ensemble, "wine", n_learners, 0.001, 2, file_name=None, plot=True, save_plot_dir="examples/", attribute=None, communities=True, class_flag=False)

    df.to_csv(class_name+"_dpg.csv")